{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "facdf180",
   "metadata": {},
   "source": [
    "# Pràctica I - Perceptron\n",
    "\n",
    "F. Rosenblatt, 1958 - Basat en el model neuronal proposat per McCulloch-Pitts.\n",
    "\n",
    "Es un algorisme que **apren de manera òptima els coeficients dels pesos** de la neurona que un cop multiplicats per les característiques d'entrada permet determinar si la neurona s'activa o no."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4a1458",
   "metadata": {},
   "source": [
    "## Teoría bàsica\n",
    "\n",
    "Aquest problema permet modelar una tasca de classificació binaria on tenim 2 classes ( 1 i -1) en la que definim una funció d'activació $\\phi(z)$ que es calcula com una combinació lineal d'un vector de característiques $\\mathbf{x}$ y un vector de pesos $\\mathbf{w}$. On $z = w_1x_1 + \\ldots + w_mx_m$.\n",
    "\n",
    "Llavors, donat un exemple $x^{(i)}$, si la sortida de $\\phi(z)$ és major que un determinat valor (_threshold_) $\\theta$ podem prediure que aquest pertany a la classe 1 i en cas contrari direm que pertany a la classe -1. això es coneix com a funció escaló (en anglés _step function_).\n",
    "\n",
    "$$\\phi(z) = \\begin{cases}\n",
    " 1 & \\text{if z} >= \\theta, \\\\\n",
    "-1 & \\text{en cas contrari}\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "A efectes pràctics, podem moure el _threshold_ a la part esquerra de l'eqüació i definir un pes $w_0 = -\\theta$ i  $x_0 = 1$  d'aquesta manera podem escriure $\\mathbf{z}$ com a:  $z = w_0x_0 + w_1x_1 + \\ldots + w_mx_m = w^Tx$ i llavors:\n",
    "\n",
    "\n",
    "$$\\phi(z) = \\begin{cases}\n",
    " 1 & \\text{if z} >= 0, \\\\\n",
    "-1 & \\text{en cas contrari}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2771d7",
   "metadata": {},
   "source": [
    "## Algorisme\n",
    "\n",
    "L'algorisme es resumeix en les següents pases:\n",
    "\n",
    "1. Inicialitzar els pesos a valor 0 (o a nombre aleatòri prou petit).\n",
    "\n",
    "2. Per cada element, $x^{(i)}$, del conjunt que emprarem per entrenar fer:\n",
    "\n",
    "    1.Calcular el valor de sortida de la xarxa $\\hat y$.\n",
    "    \n",
    "    2.Actualitzar el vector de pesos, $\\mathbf{w}$.\n",
    "     \n",
    "\n",
    "L'actualització d'un element $w_j$ del vector de pesos $w$ es pot expressar de la següent manera:\n",
    "\n",
    "$$w_j = w_j + \\Delta w_j$$\n",
    "\n",
    "El valor de $\\Delta w_j$ es calcula mitjançant la norma d'aprenentatge del perceptron:\n",
    "\n",
    "$$\\Delta w_j = \\eta(y^{(i)} - \\hat y^{(i)}) x_j^{(i)}$$\n",
    "\n",
    "on $\\eta$ és el rati d'aprenentatge (una constant entre 0.0 i 1.0), $y^{(i)}$ és la classe per l'exemple ièssim i $\\hat y^{(i)}$ és la classe que el classificador a predit.\n",
    "\n",
    "\n",
    "\n",
    "*Nota*: Aquest algorisme només convergeix si les dues classes són separables linealment i el rati d'aprenentatge és suficientment petit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f43e82",
   "metadata": {},
   "source": [
    " # Feina a fer\n",
    " \n",
    " \n",
    " 0. Entendre l'algorisme del perceptron.\n",
    " 1. Implementar els mètodes _fit_, _net\\_input_  i _predict_ de la classe Perceptron.\n",
    " 2. Entrenar un perceptron amb el conjunt de dades que trobareu en el fitxer _main_.\n",
    " 3. Validar l'entrenament mostrant la recta de divisió del conjunt. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:aprenentatge_automatic]",
   "language": "python",
   "name": "conda-env-aprenentatge_automatic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}